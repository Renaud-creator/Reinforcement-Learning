{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred> Value iteration </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkblue> Value iteration on a \"windy gridworld\" and a simple gridworld example </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value iteration is an alternative technique for solving the control problem. The objective is to be less computationaly expensive in order to find the optimal policy. Before, the policy improvement was iterative and inside the loop, policy evaluation was also iterative so that there one iterative algorithm inside another one which is computationaly expensive to run. In fact, after few iterations of policy evaluation, the policy improvement will result into the same policy if we were to wait until convergence. Value iteration combines into one step policy evaluation and policy improvement. There are now one loop for finding the optimal value function and one loop for finding the corresponding optimal policy. This is computationaly very less expensive than one loop inside another one, even if there is a problem with hundreds of states and actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ACTION_SPACE=['U','D','L','R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Windy_gridworld:\n",
    "    def __init__(self, row, col, start):\n",
    "        self.rows=row\n",
    "        self.cols=col\n",
    "        self.i=start[0]\n",
    "        self.j=start[1]\n",
    "        \n",
    "    def set(self, rewards, actions, probs):\n",
    "        self.reward = rewards\n",
    "        self.action = actions\n",
    "        self.probs = probs\n",
    "        \n",
    "    #set the state to s for the gridworld object\n",
    "    def set_state(self,s): \n",
    "        self.i=s[0]\n",
    "        self.j=s[1]\n",
    "    \n",
    "    #returns the current state of the grid\n",
    "    def current_state(self):\n",
    "        return((self.i,self.j))\n",
    "    \n",
    "    #returns True if the state s is a terminal state (i.e: not in the action dictionnary)\n",
    "    def is_terminal(self,s):\n",
    "        return(s not in self.action.keys())\n",
    "    \n",
    "    #return the reward after the action was taking into account (by default reward is 0)\n",
    "    def move(self,a):\n",
    "        s = (self.i,self.j)\n",
    "        a = action \n",
    "        \n",
    "        next_state_dict = self.probs[(s,a)]\n",
    "        next_states = list(next_state_dict.keys())\n",
    "        next_probs = list(next_state_dict.values())\n",
    "        \n",
    "        s2 = np.random.choice(next_states,1,p=next_probs)\n",
    "        self.i,self.j = s2[0],s2[1]\n",
    "        return(self.reward.get(s2,0))\n",
    "    \n",
    "    def game_over(self):\n",
    "        return((self.i,self.j) in self.reward)\n",
    "    \n",
    "    def all_states(self):\n",
    "        return(set(self.action.keys() | self.reward.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windygrid(step_cost):\n",
    "    g = Windy_gridworld(3,4,[2,0])\n",
    "    rewards = {(0,3):+1,\n",
    "               (1,3):-1,\n",
    "               (0,0):step_cost,\n",
    "               (0,1):step_cost,\n",
    "               (0,2):step_cost,\n",
    "               (1,0):step_cost,\n",
    "               (1,2):step_cost,\n",
    "               (2,0):step_cost,\n",
    "               (2,1):step_cost,\n",
    "               (2,2):step_cost,\n",
    "               (2,3):step_cost}\n",
    "    \n",
    "    actions = {(0,0):('R','D'),\n",
    "               (0,1):('R','L'),\n",
    "               (0,2):('D','R','L'),\n",
    "               (1,0):('U','D'),\n",
    "               (1,2):('U','D','R'),\n",
    "               (2,0):('U','R'),\n",
    "               (2,1):('R','L'),\n",
    "               (2,2):('U','R','L'),\n",
    "               (2,3):('U','L')}\n",
    "    probs = {\n",
    "    ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "    ((2, 0), 'D'): {(2, 0): 1.0},\n",
    "    ((2, 0), 'L'): {(2, 0): 1.0},\n",
    "    ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "    ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "    ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "    ((1, 0), 'L'): {(1, 0): 1.0},\n",
    "    ((1, 0), 'R'): {(1, 0): 1.0},\n",
    "    ((0, 0), 'U'): {(0, 0): 1.0},\n",
    "    ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "    ((0, 0), 'L'): {(0, 0): 1.0},\n",
    "    ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'U'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'D'): {(0, 1): 1.0},\n",
    "    ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "    ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "    ((0, 2), 'U'): {(0, 2): 1.0},\n",
    "    ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "    ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "    ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "    ((2, 1), 'U'): {(2, 1): 1.0},\n",
    "    ((2, 1), 'D'): {(2, 1): 1.0},\n",
    "    ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "    ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "    ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "    ((2, 2), 'D'): {(2, 2): 1.0},\n",
    "    ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "    ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "    ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "    ((2, 3), 'D'): {(2, 3): 1.0},\n",
    "    ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "    ((2, 3), 'R'): {(2, 3): 1.0},\n",
    "    ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n",
    "    ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "    ((1, 2), 'L'): {(1, 2): 1.0},\n",
    "    ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "  }\n",
    "    \n",
    "    g.set(rewards,actions,probs)\n",
    "    return(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(V,g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"----------------------------\")\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i,j),0)\n",
    "            if v>=0:\n",
    "                print(\" %.2f |\" %v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f |\" %v, end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(P,g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"----------------\")\n",
    "        for j in range(g.cols):\n",
    "            p = P.get((i,j),' ')\n",
    "            print(\" %s |\" %p, end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_probs_and_rewards(grid):\n",
    "\n",
    "    #This dictionnary gives the distributions p(s'|s,a). \n",
    "    #If the key is (s,a,s'), then transition_probs[(s,a,s')] is p(s'|s,a)\n",
    "    transition_probs={}\n",
    "\n",
    "    #We use here deterministic rewards. \n",
    "    #If the key is (s,a,s'), then rewards[(s,a,s')] is the reward of state s'\n",
    "    rewards={}\n",
    "\n",
    "    for key,value in grid.probs.items():\n",
    "        for k,v in value.items():\n",
    "            (s,a),s2,p=key,k,v\n",
    "            transition_probs[(s,a,s2)]=p\n",
    "            rewards[s,a,s2]=grid.reward.get(s2,0)\n",
    "            \n",
    "    return(transition_probs,rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "----------------------------\n",
      "-0.10 |-0.10 |-0.10 | 1.00 |\n",
      "----------------------------\n",
      "-0.10 | 0.00 |-0.10 |-1.00 |\n",
      "----------------------------\n",
      "-0.10 |-0.10 |-0.10 |-0.10 |\n",
      "values:\n",
      "----------------------------\n",
      " 0.62 | 0.80 | 1.00 | 0.00 |\n",
      "----------------------------\n",
      " 0.46 | 0.00 |-0.04 | 0.00 |\n",
      "----------------------------\n",
      " 0.31 | 0.18 | 0.06 |-0.04 |\n",
      "policy:\n",
      "----------------\n",
      " R | R | R |   |\n",
      "----------------\n",
      " U |   | D |   |\n",
      "----------------\n",
      " U | L | L | L |\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.001\n",
    "gamma=0.9\n",
    "grid = windygrid(-0.1)\n",
    "transition_probs, rewards = get_transition_probs_and_rewards(grid)\n",
    "\n",
    "# print rewards\n",
    "print(\"rewards:\")\n",
    "print_value(grid.reward, grid)\n",
    "\n",
    "# initialize V(s)\n",
    "V = {}\n",
    "for s in grid.all_states():\n",
    "    V[s] = 0\n",
    "\n",
    "# repeat until convergence\n",
    "# V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
    "it = 0\n",
    "while True:\n",
    "    biggest_change = 0\n",
    "    for s in grid.all_states():\n",
    "        if not grid.is_terminal(s):\n",
    "            old_v = V[s]\n",
    "            new_v = float('-inf')\n",
    "\n",
    "            for a in ACTION_SPACE:\n",
    "                v = 0\n",
    "                for s2 in grid.all_states():\n",
    "                    # reward is 0 if not specified\n",
    "                    r = rewards.get((s, a, s2), 0)\n",
    "                    v += transition_probs.get((s, a, s2), 0) * (r + gamma * V[s2])\n",
    "\n",
    "                  # keep v if it's better\n",
    "                if v > new_v:\n",
    "                    new_v = v\n",
    "\n",
    "            V[s] = new_v\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    it += 1\n",
    "    if biggest_change < threshold:\n",
    "        break\n",
    "\n",
    "# find a policy that leads to optimal value function\n",
    "policy = {}\n",
    "for s in grid.action.keys():\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "    # loop through all possible actions to find the best current action\n",
    "    for a in ACTION_SPACE:\n",
    "        v = 0\n",
    "        for s2 in grid.all_states():\n",
    "            # reward is a function of (s, a, s'), 0 if not specified\n",
    "            r = rewards.get((s, a, s2), 0)\n",
    "            v += transition_probs.get((s, a, s2), 0) * (r + gamma * V[s2])\n",
    "\n",
    "        # best_a is the action associated with best_value\n",
    "        if v > best_value:\n",
    "            best_value = v\n",
    "            best_a = a\n",
    "    policy[s] = best_a\n",
    "\n",
    "# our goal here is to verify that we get the same answer as with policy iteration\n",
    "print(\"values:\")\n",
    "print_value(V, grid)\n",
    "print(\"policy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkblue> Conclusion and remarks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that value iteration leads to the same policy as the previous method which was to alternate between policy improvement and policy iteration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
